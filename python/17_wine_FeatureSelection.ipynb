{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the right features(variables) can improve the learning process in data science by reducing the amount of noise (useless information) that can influence the learner’s estimates. Variable selection, therefore, can effectively reduce the variance of predictions. In order to involve just the useful variables in training and leave out the redundant ones, you can use these techniques:\n",
    "\n",
    "**Univariate Approach**: Select the variables most related to the target outcome.\n",
    "\n",
    "**Greedy or Backward Approach**: Keep only the variables that you can remove from the learning process without damaging its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting by univariate measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you decide to select a variable by its level of association with its target, the <font color='blue'> **sklearn.feature_selection.SelectPercentile** </font> provides an automatic procedure for keeping only a certain percentage of the best, associated features. The available metrics for association are\n",
    "\n",
    "**f_regression**: Used only for numeric targets and based on linear regression performance.\n",
    "\n",
    "**f_classif**: Used only for categorical targets and based on the Analysis of Variance (ANOVA) statistical test.\n",
    "\n",
    "**chi2**: Performs the chi-square statistic for categorical targets, which is less sensible to the nonlinear relationship between the predictive variable and its target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating candidates for a classification problem, **f_classif** and **chi2** tend to provide the same set of top variables. It’s still a good practice to test the selections from both the association metrics.\n",
    "\n",
    "Apart from applying a direct selection of the top percentile associations, SelectPercentile can also rank the best variables to make it easier to decide at what percentile to exclude a feature from participating in the learning process. The <font color='blue'> **sklearn.feature_selection.SelectKBest** </font> is analogous in its functionality, but it selects the top **k** variables, where k is a number, not a percentile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the level of association output helps you to choose the most important variables for your machine-learning model, but you should watch out for these possible problems:\n",
    "\n",
    "- Some variables with high association could also be highly correlated, introducing duplicated information, which     acts as noise in the learning process.\n",
    "\n",
    "- Some variables may be penalized, especially binary ones (variables indicating a status or characteristic using the value 1 when it is present, 0 when it is not). For example, notice that the output shows the binary variable CHAS as the least associated with the target variable (but you know from previous examples that it’s influential from the cross-validation phase).\n",
    "\n",
    "The univariate selection process can give you a **real advantage** when you have **a huge number of variables** to select from and all other methods turn computationally infeasible. The best procedure is to reduce the value of **SelectPercentile** by half or more of the available variables, reduce the number of variables to a manageable number, and consequently allow the use of a more sophisticated and more precise method such as a greedy search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a greedy search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a univariate selection, you have to decide for yourself how many variables to keep: Greedy selection automatically reduces the number of features involved in a learning model on the basis of their effective contribution to the performance measured by the error measure.\n",
    "\n",
    "The RFECV class, fitting the data, can provide you with information on the number of useful features, point them out to you, and automatically transform the X data, by the method transform, into a reduced variable set, as shown in the following example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s possible to obtain an index to the optimum variable set by calling the attribute **support_** from the RFECV class after you fit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that CHAS is now included among the most predictive features, which contrasts with the result from the univariate search. The RFECV method can detect whether a variable is important, no matter whether it is binary, categorical, or numeric, because it directly evaluates the role played by the feature in the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RFECV method is certainly more efficient, when compared to the univariate approach, because it considers highly correlated features and is tuned to optimize the evaluation measure (which usually is not Chi-square or F-score). Being a greedy process, it’s computationally demanding and may only approximate the best set of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As RFECV learns the best set of variables from data, the selection may overfit, which is what happens with all other machine-learning algorithms. Trying RFECV on different samples of the training data can confirm the best variables to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DFr = pd.read_csv(\"winequality-red.csv\")\n",
    "DFw = pd.read_csv(\"winequality-white.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599, 4898)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xr = DFr.ix[:,0:10].as_matrix()\n",
    "yr = DFr.ix[:,  11].as_matrix()\n",
    "\n",
    "Xw = DFw.ix[:,0:10].as_matrix()\n",
    "yw = DFw.ix[:,  11].as_matrix()\n",
    "len(yr), len(yw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.2606523696 0.0464500416471\n",
      "15.5802890515 0.00815035154205\n",
      "13.0256651036 0.0231394417254\n",
      "4.12329473592 0.531804674961\n",
      "0.75242557946 0.979968039781\n",
      "161.936036048 3.82728810062e-33\n",
      "2755.55798423 0.0\n",
      "0.000230432045444 0.999999999957\n",
      "0.154654735634 0.999526491006\n",
      "4.5584877468 0.47209632134\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "chi, pvalue1 = chi2(Xr, yr)\n",
    "\n",
    "for i in range(len(Xr[0])):\n",
    "    print chi[i], pvalue1[i]\n",
    "\n",
    "#selector = SelectPercentile(f_classif, percentile=25)\n",
    "#selector.fit(Xr, yr)\n",
    "#print selector.pvalues_\n",
    "#print selector.scores_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.28308115822 8.79396662384e-06\n",
      "60.9139928316 3.32646506052e-58\n",
      "19.6906644662 4.4210915745e-19\n",
      "1.05337357785 0.384618775429\n",
      "6.03563859236 1.52653902486e-05\n",
      "4.75423310399 0.000257082723402\n",
      "25.4785095183 8.53359844527e-25\n",
      "13.3963569138 8.12439442344e-13\n",
      "4.34176430319 0.000628438870133\n",
      "22.2733760896 1.22589009185e-21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "fvalue, pvalue = f_classif(Xr, yr)\n",
    "\n",
    "for i in range(len(Xr[0])):\n",
    "    print fvalue[i], pvalue[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.79396662384e-06 6.28308115822\n",
      "3.32646506052e-58 60.9139928316\n",
      "4.4210915745e-19 19.6906644662\n",
      "0.384618775429 1.05337357785\n",
      "1.52653902486e-05 6.03563859236\n",
      "0.000257082723402 4.75423310399\n",
      "8.53359844527e-25 25.4785095183\n",
      "8.12439442344e-13 13.3963569138\n",
      "0.000628438870133 4.34176430319\n",
      "1.22589009185e-21 22.2733760896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "selector = SelectPercentile(f_classif, percentile=25)\n",
    "selector.fit(Xr, yr)\n",
    "\n",
    "for i in range(len(selector.pvalues_)):\n",
    "    print selector.pvalues_[i], selector.scores_[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The classifier does not expose \"coef_\" or \"feature_importances_\" attributes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-85e800bb6586>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m                    scoring = 'roc_auc')\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/rostam/.local/lib/python2.7/site-packages/sklearn/feature_selection/rfe.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    422\u001b[0m                       verbose=self.verbose - 1)\n\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m             rfe._fit(X_train, y_train, lambda estimator, features:\n\u001b[0m\u001b[0;32m    425\u001b[0m                      _score(estimator, X_test[:, features], y_test, scorer))\n\u001b[0;32m    426\u001b[0m             \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrfe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/rostam/.local/lib/python2.7/site-packages/sklearn/feature_selection/rfe.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, step_score)\u001b[0m\n\u001b[0;32m    180\u001b[0m                 \u001b[0mcoefs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m                 raise RuntimeError('The classifier does not expose '\n\u001b[0m\u001b[0;32m    183\u001b[0m                                    \u001b[1;34m'\"coef_\" or \"feature_importances_\" '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                                    'attributes')\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The classifier does not expose \"coef_\" or \"feature_importances_\" attributes"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC()\n",
    "selector = RFECV(estimator = clf,\n",
    "                        cv = StratifiedKFold(yr, 3),\n",
    "                   scoring = 'roc_auc')\n",
    "\n",
    "selector.fit(Xr, yr)\n",
    "\n",
    "print selector.estimator_\n",
    "print selector.n_features_\n",
    "print selector.support_\n",
    "print selector.ranking_\n",
    "print selector.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-083e0f019818>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpcat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"mle\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhiten\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mXrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpcaf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mXrt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpcat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pcaf = PCA(copy=True, n_components=\"mle\", whiten=False)\n",
    "pcat = PCA(copy=True, n_components=\"mle\", whiten=True)\n",
    "\n",
    "Xrf = pcaf.fit_transform(X)\n",
    "Xrt = pcat.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e0ad952d3e3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m                         \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                    scoring = \"mean_squared_error\")\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "regression = linear_model.LinearRegression()\n",
    "selector = RFECV(estimator = regression,\n",
    "                        cv = 10,\n",
    "                   scoring = \"mean_squared_error\")\n",
    "selector.fit(X, y)\n",
    "\n",
    "print selector.estimator_\n",
    "print selector.n_features_\n",
    "print selector.support_\n",
    "print selector.ranking_\n",
    "print selector.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7002 (+/- 0.4422)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(loss = 'squared_hinge')\n",
    "\n",
    "kfold = KFold(len(X), n_folds = 5)\n",
    "scores= cross_val_score(clf, X, y, cv=kfold, n_jobs=-1)\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" %(np.mean(scores), np.sqrt(np.std(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9384 (+/- 0.2027)\n"
     ]
    }
   ],
   "source": [
    "scores= cross_val_score(clf, Xrf, y, cv=kfold, n_jobs=-1)\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" %(np.mean(scores), np.sqrt(np.std(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9495 (+/- 0.1814)\n"
     ]
    }
   ],
   "source": [
    "scores= cross_val_score(clf, Xrt, y, cv=kfold, n_jobs=-1)\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" %(np.mean(scores), np.sqrt(np.std(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1060 (+/- 0.3811)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "\n",
    "kfold = KFold(len(X), n_folds = 5)\n",
    "scores= cross_val_score(clf, X, y, cv=kfold, n_jobs=-1)\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" %(np.mean(scores), np.sqrt(np.std(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1005 (+/- 0.3841)\n"
     ]
    }
   ],
   "source": [
    "scores= cross_val_score(clf, Xrf, y, cv=kfold, n_jobs=-1)\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" %(np.mean(scores), np.sqrt(np.std(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8646 (+/- 0.3082)\n"
     ]
    }
   ],
   "source": [
    "scores= cross_val_score(clf, Xrt, y, cv=kfold, n_jobs=-1)\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" %(np.mean(scores), np.sqrt(np.std(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "kNN = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6100 (+/- 0.5566)\n"
     ]
    }
   ],
   "source": [
    "scores= cross_val_score(kNN, X, y, cv=kfold, n_jobs=-1)\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" %(np.mean(scores), np.sqrt(np.std(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6100 (+/- 0.5566)\n"
     ]
    }
   ],
   "source": [
    "scores= cross_val_score(kNN, Xrf, y, cv=kfold, n_jobs=-1)\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" %(np.mean(scores), np.sqrt(np.std(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6106 (+/- 0.4637)\n"
     ]
    }
   ],
   "source": [
    "scores= cross_val_score(kNN, Xrt, y, cv=kfold, n_jobs=-1)\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" %(np.mean(scores), np.sqrt(np.std(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit(X[, y])           Fit the model with X.\n",
    "# fit_transform(X[, y]) Fit the model with X and apply the dimensionality reduction on X.\n",
    "# get_covariance()      Compute data covariance with the generative model.\n",
    "# get_params([deep])    Get parameters for this estimator.\n",
    "# get_precision()       Compute data precision matrix with the generative model.\n",
    "# inverse_transform(X)  Transform data back to its original space, i.e.,\n",
    "# score(X[, y])         Return the average log-likelihood of all samples\n",
    "# score_samples(X)      Return the log-likelihood of each sample\n",
    "# set_params(**params)  Set the parameters of this estimator.\n",
    "# transform(X)          Apply the dimensionality reduction on X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA(copy=True, n_components=1, whiten=True)\n",
      "Accuracy: 0.5487 (+/- 0.5617)\n",
      "0.998091230492\n",
      "15.7208047352\n",
      "\n",
      "PCA(copy=True, n_components=2, whiten=True)\n",
      "Accuracy: 0.5483 (+/- 0.5372)\n",
      "0.999827146117\n",
      "1.55306269038\n",
      "\n",
      "PCA(copy=True, n_components=3, whiten=True)\n",
      "Accuracy: 0.6208 (+/- 0.5672)\n",
      "0.999922105074\n",
      "0.769859900136\n",
      "\n",
      "PCA(copy=True, n_components=4, whiten=True)\n",
      "Accuracy: 0.8986 (+/- 0.1979)\n",
      "0.99997232243\n",
      "0.303940080331\n",
      "\n",
      "PCA(copy=True, n_components=5, whiten=True)\n",
      "Accuracy: 0.8986 (+/- 0.2309)\n",
      "0.999984686115\n",
      "0.189189889935\n",
      "\n",
      "PCA(copy=True, n_components=6, whiten=True)\n",
      "Accuracy: 0.8979 (+/- 0.2905)\n",
      "0.999993148245\n",
      "0.0967400468496\n",
      "\n",
      "PCA(copy=True, n_components=7, whiten=True)\n",
      "Accuracy: 0.8983 (+/- 0.2800)\n",
      "0.99999595506\n",
      "0.0666290119774\n",
      "\n",
      "PCA(copy=True, n_components=8, whiten=True)\n",
      "Accuracy: 0.8978 (+/- 0.3478)\n",
      "0.99999747814\n",
      "0.0498486524068\n",
      "\n",
      "PCA(copy=True, n_components=9, whiten=True)\n",
      "Accuracy: 0.8644 (+/- 0.3264)\n",
      "0.999998605971\n",
      "0.0344440636005\n",
      "\n",
      "PCA(copy=True, n_components=10, whiten=True)\n",
      "Accuracy: 0.8870 (+/- 0.3250)\n",
      "0.999999327387\n",
      "0.022158824951\n",
      "\n",
      "PCA(copy=True, n_components=11, whiten=True)\n",
      "Accuracy: 0.8763 (+/- 0.2705)\n",
      "0.999999705447\n",
      "0.0145557984959\n",
      "\n",
      "PCA(copy=True, n_components=12, whiten=True)\n",
      "Accuracy: 0.8646 (+/- 0.3082)\n",
      "0.999999917461\n",
      "0.00815761492188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 13):\n",
    "    pcai = PCA(n_components=i, whiten=True)\n",
    "    print pcai\n",
    "    Xri = pcai.fit_transform(X)\n",
    "    scores = cross_val_score(clf, Xri, y, cv=kfold, n_jobs=-1)\n",
    "    print(\"Accuracy: %0.4f (+/- %0.4f)\" %(np.mean(scores), np.sqrt(np.std(scores))))\n",
    "    print(sum(pcai.explained_variance_ratio_))\n",
    "    print(pcai.noise_variance_)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
